import cv2
import time
import numpy as np
from collections import deque
import mediapipe as mp
from typing import Optional, Callable
import math


class EnhancedVisionRecognition:
    """
    è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«æ¨¡å—
    é›†æˆåŠŸèƒ½ï¼š
    1. æ‰‹åŠ¿è¯†åˆ« - æ§åˆ¶éŸ³ä¹ã€ç©ºè°ƒç­‰
    2. å¤´éƒ¨åŠ¨ä½œè¯†åˆ« - ç¡®è®¤/å–æ¶ˆæ“ä½œ
    3. çœ¼éƒ¨çŠ¶æ€ç›‘æ§ - é©¾é©¶å‘˜æ³¨æ„åŠ›æ£€æµ‹
    """

    def __init__(self, command_callback: Optional[Callable[[str, str], None]] = None):
        self.command_callback = command_callback or self.default_callback

        # ===== MediaPipe åˆå§‹åŒ– =====
        self.mp_hands = mp.solutions.hands
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        # æ‰‹éƒ¨æ£€æµ‹å™¨
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )

        # é¢éƒ¨æ£€æµ‹å™¨
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )

        # ===== æ‰‹åŠ¿è¯†åˆ«å‚æ•° =====
        self.finger_threshold = 0.02  # æ‰‹æŒ‡ä¼¸ç›´é˜ˆå€¼
        self.gesture_stability_frames = 5  # æ‰‹åŠ¿ç¨³å®šç¡®è®¤å¸§æ•°
        self.gesture_history = deque(maxlen=self.gesture_stability_frames)
        self.current_gesture = "None"

        # ===== å¤´éƒ¨åŠ¨ä½œè¯†åˆ«å‚æ•° =====
        self.head_movement_threshold = 0.1  # å¤´éƒ¨ç§»åŠ¨é˜ˆå€¼ï¼ˆé™ä½ä»¥æé«˜æ•æ„Ÿåº¦ï¼‰
        self.nod_threshold = 0.1  # ç‚¹å¤´ä¸“ç”¨é˜ˆå€¼ï¼ˆæ›´æ•æ„Ÿï¼‰
        self.head_action_frames = 10  # å¤´éƒ¨åŠ¨ä½œç¡®è®¤å¸§æ•°ï¼ˆå¢åŠ ä»¥è·å¾—æ›´å¥½çš„æ£€æµ‹ï¼‰
        self.head_movement_history = deque(maxlen=20)  # å¢åŠ å†å²è®°å½•
        self.head_action_history = deque(maxlen=self.head_action_frames)
        self.current_head_action = "None"

        # ===== çœ¼éƒ¨çŠ¶æ€ç›‘æ§å‚æ•° =====
        self.eye_aspect_ratio_threshold = 0.21  # çœ¼ç›é—­åˆé˜ˆå€¼
        self.eye_closed_frames_threshold = 60  # é—­çœ¼å¸§æ•°é˜ˆå€¼ (çº¦2ç§’ @30fps)
        self.consecutive_closed_frames = 0
        self.eyes_status = "Open"
        self.driver_attention_status = "Normal"

        # ===== æŒ‡ä»¤æ§åˆ¶å‚æ•° =====
        self.command_cooldown = 2.0  # æŒ‡ä»¤å†·å´æ—¶é—´
        self.last_command_time = 0
        self.last_head_command_time = 0
        self.last_attention_alert_time = 0

        # ===== ç³»ç»ŸçŠ¶æ€ =====
        self.is_running = False
        self.frame_count = 0

        # ===== æŒ‡ä»¤æ˜ å°„é…ç½® =====
        self.gesture_commands = {
            'Open Palm': 'æ’­æ”¾éŸ³ä¹',
            'Fist': 'æš‚åœéŸ³ä¹',
            'Index Up': 'å‡æ¸©',
            'Two Fingers Up': 'é™æ¸©'
        }

        self.head_action_commands = {
            'Nod': 'ç¡®è®¤æ“ä½œ',
            'Shake': 'å–æ¶ˆæ“ä½œ'
        }

        # ===== é¢éƒ¨å…³é”®ç‚¹ç´¢å¼• =====
        self.face_landmarks_indices = {
            'nose_tip': 1,
            'left_eye': [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246],
            'right_eye': [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398],
            'left_eye_corners': [33, 133],
            'right_eye_corners': [362, 263]
        }

        self._print_startup_info()

    def _print_startup_info(self):
        """æ‰“å°å¯åŠ¨ä¿¡æ¯"""
        print("ğŸ¯ è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿå·²å¯åŠ¨")
        print("=" * 50)
        print("âœ‹ æ‰‹åŠ¿è¯†åˆ«åŠŸèƒ½:")
        for gesture, command in self.gesture_commands.items():
            print(f"   {gesture} â†’ {command}")
        print("\nğŸ¤– å¤´éƒ¨åŠ¨ä½œè¯†åˆ«åŠŸèƒ½:")
        for action, command in self.head_action_commands.items():
            print(f"   {action} â†’ {command}")
        print("\nğŸ‘ï¸ é©¾é©¶å‘˜æ³¨æ„åŠ›ç›‘æ§:")
        print("   é—­çœ¼è¶…è¿‡2ç§’ â†’ åˆ†å¿ƒè­¦å‘Š")
        print("\nâš™ï¸ æ£€æµ‹å‚æ•°:")
        print(f"   ç‚¹å¤´æ£€æµ‹é˜ˆå€¼: {self.nod_threshold}")
        print(f"   æ‘‡å¤´æ£€æµ‹é˜ˆå€¼: {self.head_movement_threshold}")
        print(f"   å¤´éƒ¨åŠ¨ä½œç¡®è®¤å¸§æ•°: {self.head_action_frames}")
        print("=" * 50)
        print("ğŸ’¡ è°ƒè¯•æç¤º: è§‚å¯Ÿç•Œé¢ä¸‹æ–¹çš„ Y_range å’Œ X_range æ•°å€¼")
        print("   ç‚¹å¤´æ—¶ Y_range åº”è¯¥è¶…è¿‡ç‚¹å¤´é˜ˆå€¼")
        print("   æ‘‡å¤´æ—¶ X_range åº”è¯¥è¶…è¿‡æ‘‡å¤´é˜ˆå€¼")
        print("\nğŸ”§ å®æ—¶è°ƒè¯•æŒ‰é”®:")
        print("   1/2: è°ƒæ•´ç‚¹å¤´é˜ˆå€¼ï¼ˆå‡å°‘/å¢åŠ ï¼‰")
        print("   3/4: è°ƒæ•´æ‘‡å¤´é˜ˆå€¼ï¼ˆå‡å°‘/å¢åŠ ï¼‰")
        print("   R: é‡ç½®æ‰€æœ‰å‚æ•°")
        print("   Q/ESC: é€€å‡ºç³»ç»Ÿ")

    def default_callback(self, cmd_type: str, cmd_text: str):
        """é»˜è®¤å›è°ƒå‡½æ•°"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] ğŸ¯ {cmd_type}: {cmd_text}")

    # =================== æ‰‹åŠ¿è¯†åˆ«æ¨¡å— ===================

    def detect_gesture(self, hands_results):
        """
        æ‰‹åŠ¿è¯†åˆ«æ ¸å¿ƒç®—æ³•
        æ”¯æŒï¼šå¼ å¼€æ‰‹æŒã€æ¡æ‹³ã€é£ŸæŒ‡å‘ä¸Šã€é£ŸæŒ‡+ä¸­æŒ‡å‘ä¸Š
        """
        if not hands_results.multi_hand_landmarks:
            return "None"

        landmarks = hands_results.multi_hand_landmarks[0].landmark

        # è·å–æ‰‹æŒ‡å…³é”®ç‚¹
        finger_landmarks = {
            'thumb': {'tip': landmarks[4], 'pip': landmarks[3]},
            'index': {'tip': landmarks[8], 'pip': landmarks[6]},
            'middle': {'tip': landmarks[12], 'pip': landmarks[10]},
            'ring': {'tip': landmarks[16], 'pip': landmarks[14]},
            'pinky': {'tip': landmarks[20], 'pip': landmarks[18]}
        }

        # åˆ¤æ–­æ¯ä¸ªæ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´
        fingers_extended = []

        # æ‹‡æŒ‡ç‰¹æ®Šå¤„ç†ï¼ˆæ°´å¹³ä¼¸å±•ï¼‰
        thumb_extended = abs(finger_landmarks['thumb']['tip'].x -
                             finger_landmarks['thumb']['pip'].x) > self.finger_threshold
        fingers_extended.append(thumb_extended)

        # å…¶ä»–å››æŒ‡ï¼ˆå‚ç›´ä¼¸å±•ï¼‰
        for finger_name in ['index', 'middle', 'ring', 'pinky']:
            tip = finger_landmarks[finger_name]['tip']
            pip = finger_landmarks[finger_name]['pip']
            extended = tip.y < pip.y - self.finger_threshold
            fingers_extended.append(extended)

        # è°ƒè¯•è¾“å‡ºï¼ˆæ¯20å¸§ä¸€æ¬¡ï¼‰
        if self.frame_count % 20 == 0:
            finger_names = ['T', 'I', 'M', 'R', 'P']
            finger_status = [f"{name}:{int(ext)}" for name, ext in zip(finger_names, fingers_extended)]
            print(f"æ‰‹æŒ‡çŠ¶æ€: [{', '.join(finger_status)}]")

        # æ‰‹åŠ¿è¯†åˆ«é€»è¾‘
        extended_count = sum(fingers_extended)

        if extended_count >= 4:
            return "Open Palm"
        elif extended_count <= 1:
            return "Fist"
        elif (fingers_extended[1] and fingers_extended[2] and
              not fingers_extended[3] and not fingers_extended[4]):
            return "Two Fingers Up"
        elif (fingers_extended[1] and not fingers_extended[2] and
              not fingers_extended[3] and not fingers_extended[4]):
            return "Index Up"
        else:
            return "None"

    def process_gesture_stable(self, raw_gesture):
        """æ‰‹åŠ¿ç¨³å®šæ€§å¤„ç†"""
        self.gesture_history.append(raw_gesture)

        if len(self.gesture_history) < self.gesture_stability_frames:
            return self.current_gesture

        # ç»Ÿè®¡æœ€è¿‘æ‰‹åŠ¿
        recent_gestures = list(self.gesture_history)
        gesture_counts = {}
        for g in recent_gestures:
            gesture_counts[g] = gesture_counts.get(g, 0) + 1

        if gesture_counts:
            most_common = max(gesture_counts, key=gesture_counts.get)
            required_count = self.gesture_stability_frames // 2 + 1
            if gesture_counts[most_common] >= required_count:
                return most_common

        return self.current_gesture

    def execute_gesture_command(self, gesture):
        """æ‰§è¡Œæ‰‹åŠ¿æŒ‡ä»¤"""
        current_time = time.time()

        if (gesture != "None" and
                gesture != self.current_gesture and
                current_time - self.last_command_time > self.command_cooldown):

            if gesture in self.gesture_commands:
                command = self.gesture_commands[gesture]
                self.command_callback('æ‰‹åŠ¿', command)
                self.last_command_time = current_time
                print(f"âœ… æ‰‹åŠ¿æŒ‡ä»¤: {gesture} â†’ {command}")

    # =================== å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æ¨¡å— ===================

    def detect_head_action(self, face_results):
        """
        å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æ ¸å¿ƒç®—æ³•
        æ”¯æŒï¼šç‚¹å¤´ã€æ‘‡å¤´
        """
        if not face_results.multi_face_landmarks:
            return "None"

        face_landmarks = face_results.multi_face_landmarks[0]
        landmarks = face_landmarks.landmark

        try:
            # ä½¿ç”¨é¼»å°–ä½œä¸ºå¤´éƒ¨ä½ç½®å‚è€ƒç‚¹
            nose_tip = landmarks[self.face_landmarks_indices['nose_tip']]
            current_position = (nose_tip.x, nose_tip.y)

            self.head_movement_history.append(current_position)

            if len(self.head_movement_history) < self.head_action_frames:
                return "None"

            # åˆ†æå¤´éƒ¨ç§»åŠ¨æ¨¡å¼
            positions = list(self.head_movement_history)

            # Yè½´å˜åŒ–åˆ†æï¼ˆç‚¹å¤´ï¼‰- å›¾åƒåæ ‡ç³»Yå‘ä¸‹é€’å¢
            y_positions = [pos[1] for pos in positions]
            y_range = max(y_positions) - min(y_positions)

            # Xè½´å˜åŒ–åˆ†æï¼ˆæ‘‡å¤´ï¼‰
            x_positions = [pos[0] for pos in positions]
            x_range = max(x_positions) - min(x_positions)

            # è°ƒè¯•è¾“å‡ºï¼ˆæ¯30å¸§ä¸€æ¬¡ï¼‰
            if self.frame_count % 30 == 0:
                print(f"å¤´éƒ¨ç§»åŠ¨åˆ†æ: Y_range={y_range:.4f}(é˜ˆå€¼:{self.nod_threshold:.4f}), X_range={x_range:.4f}")
                print(f"Yä½ç½®å˜åŒ–: {[f'{y:.3f}' for y in y_positions[-8:]]}")
                if y_range > self.nod_threshold:
                    max_y_idx = y_positions.index(max(y_positions))
                    print(f"æ£€æµ‹åˆ°Yè½´å˜åŒ–ï¼Œæœ€ä½ç‚¹ä½ç½®: {max_y_idx}/{len(y_positions)}")

            # ç‚¹å¤´æ£€æµ‹ - ä½¿ç”¨ä¸“ç”¨é˜ˆå€¼
            if y_range > self.nod_threshold:  # ä½¿ç”¨æ›´æ•æ„Ÿçš„ç‚¹å¤´é˜ˆå€¼
                # æ–¹æ³•1ï¼šå¯»æ‰¾ç‚¹å¤´æ¨¡å¼
                max_y_idx = y_positions.index(max(y_positions))
                min_y_idx = y_positions.index(min(y_positions))

                # ç‚¹å¤´æ¨¡å¼ï¼šæœ€ä½ç‚¹åœ¨ä¸­é—´éƒ¨åˆ†ï¼Œä¸”æœ‰æ˜æ˜¾çš„ä¸‹é™å†ä¸Šå‡
                if 3 <= max_y_idx <= len(y_positions) - 4:
                    start_y = y_positions[0]
                    end_y = y_positions[-1]
                    max_y = y_positions[max_y_idx]

                    if (max_y > start_y + self.nod_threshold * 0.6 and
                            max_y > end_y + self.nod_threshold * 0.6):
                        return "Nod"

                # æ–¹æ³•2ï¼šç®€åŒ–çš„ç‚¹å¤´æ£€æµ‹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å…ˆä¸‹åä¸Šæ¨¡å¼
                mid_point = len(y_positions) // 2
                first_quarter = y_positions[:mid_point]
                second_quarter = y_positions[mid_point:]

                if len(first_quarter) >= 3 and len(second_quarter) >= 3:
                    # å‰åŠæ®µå¹³å‡å€¼vsååŠæ®µå¹³å‡å€¼ï¼Œä»¥åŠæ•´ä½“å˜åŒ–
                    first_avg = sum(first_quarter) / len(first_quarter)
                    second_avg = sum(second_quarter) / len(second_quarter)

                    # å‰åŠæ®µYå€¼åº”è¯¥å¢å¤§ï¼ˆå¤´å‘ä¸‹ï¼‰ï¼ŒååŠæ®µYå€¼åº”è¯¥å‡å°ï¼ˆå¤´å‘ä¸Šï¼‰
                    if (max(first_quarter) > min(first_quarter) + self.nod_threshold * 0.4 and
                            max(second_quarter) > min(second_quarter) + self.nod_threshold * 0.4 and
                            first_avg < second_avg):  # å‰åŠæ®µå¹³å‡ä½ç½®é«˜äºååŠæ®µ
                        return "Nod"

            # æ‘‡å¤´æ£€æµ‹ - ä¿æŒåŸæœ‰é€»è¾‘
            if x_range > self.head_movement_threshold:
                # æ£€æµ‹å·¦å³è¿åŠ¨ä¸­çš„æ–¹å‘å˜åŒ–
                direction_changes = 0
                for i in range(1, len(x_positions) - 1):
                    if ((x_positions[i] > x_positions[i - 1] and x_positions[i] > x_positions[i + 1]) or
                            (x_positions[i] < x_positions[i - 1] and x_positions[i] < x_positions[i + 1])):
                        direction_changes += 1

                # æ‘‡å¤´éœ€è¦è‡³å°‘2æ¬¡æ–¹å‘å˜åŒ–
                if direction_changes >= 2:
                    return "Shake"

            return "None"

        except (IndexError, AttributeError):
            return "None"

    def process_head_action_stable(self, raw_action):
        """å¤´éƒ¨åŠ¨ä½œç¨³å®šæ€§å¤„ç†"""
        self.head_action_history.append(raw_action)

        if len(self.head_action_history) < self.head_action_frames // 2:
            return self.current_head_action

        recent_actions = list(self.head_action_history)
        action_counts = {}
        for a in recent_actions:
            action_counts[a] = action_counts.get(a, 0) + 1

        if action_counts:
            most_common = max(action_counts, key=action_counts.get)
            required_count = len(recent_actions) // 3 + 1
            if action_counts[most_common] >= required_count and most_common != "None":
                return most_common

        return "None"

    def execute_head_command(self, action):
        """æ‰§è¡Œå¤´éƒ¨åŠ¨ä½œæŒ‡ä»¤"""
        current_time = time.time()

        if (action != "None" and
                action != self.current_head_action and
                current_time - self.last_head_command_time > self.command_cooldown):

            if action in self.head_action_commands:
                command = self.head_action_commands[action]
                self.command_callback('å¤´éƒ¨åŠ¨ä½œ', command)
                self.last_head_command_time = current_time
                print(f"âœ… å¤´éƒ¨åŠ¨ä½œ: {action} â†’ {command}")

    # =================== çœ¼éƒ¨çŠ¶æ€ç›‘æ§æ¨¡å— ===================

    def calculate_eye_aspect_ratio(self, eye_landmarks):
        """è®¡ç®—çœ¼ç›å®½é«˜æ¯” (Eye Aspect Ratio - EAR)"""
        # å‚ç›´è·ç¦»
        A = np.linalg.norm(np.array([eye_landmarks[1].x, eye_landmarks[1].y]) -
                           np.array([eye_landmarks[5].x, eye_landmarks[5].y]))
        B = np.linalg.norm(np.array([eye_landmarks[2].x, eye_landmarks[2].y]) -
                           np.array([eye_landmarks[4].x, eye_landmarks[4].y]))

        # æ°´å¹³è·ç¦»
        C = np.linalg.norm(np.array([eye_landmarks[0].x, eye_landmarks[0].y]) -
                           np.array([eye_landmarks[3].x, eye_landmarks[3].y]))

        # EAR = (A + B) / (2.0 * C)
        ear = (A + B) / (2.0 * C)
        return ear

    def detect_eye_status(self, face_results):
        """çœ¼éƒ¨çŠ¶æ€æ£€æµ‹"""
        if not face_results.multi_face_landmarks:
            return "Unknown"

        face_landmarks = face_results.multi_face_landmarks[0]
        landmarks = face_landmarks.landmark

        try:
            # è·å–å·¦å³çœ¼å…³é”®ç‚¹
            left_eye_points = [landmarks[i] for i in [33, 7, 163, 144, 145, 153]]
            right_eye_points = [landmarks[i] for i in [362, 382, 381, 380, 374, 373]]

            # è®¡ç®—åŒçœ¼EAR
            left_ear = self.calculate_eye_aspect_ratio(left_eye_points)
            right_ear = self.calculate_eye_aspect_ratio(right_eye_points)
            avg_ear = (left_ear + right_ear) / 2.0

            # çœ¼éƒ¨çŠ¶æ€åˆ¤æ–­
            if avg_ear < self.eye_aspect_ratio_threshold:
                self.consecutive_closed_frames += 1
                if self.consecutive_closed_frames >= self.eye_closed_frames_threshold:
                    return "Closed_Long"  # é•¿æ—¶é—´é—­çœ¼
                else:
                    return "Closed"  # çŸ­æ—¶é—´é—­çœ¼
            else:
                self.consecutive_closed_frames = 0
                return "Open"  # ççœ¼

        except (IndexError, AttributeError):
            return "Unknown"

    def check_driver_attention(self, eye_status):
        """é©¾é©¶å‘˜æ³¨æ„åŠ›çŠ¶æ€æ£€æŸ¥"""
        current_time = time.time()

        if eye_status == "Closed_Long":
            if self.driver_attention_status != "Distracted":
                self.driver_attention_status = "Distracted"
                # é˜²æ­¢é¢‘ç¹è­¦å‘Š
                if current_time - self.last_attention_alert_time > 5.0:
                    self.command_callback('æ³¨æ„åŠ›è­¦å‘Š', 'æ£€æµ‹åˆ°é©¾é©¶å‘˜åˆ†å¿ƒ - é•¿æ—¶é—´é—­çœ¼')
                    self.last_attention_alert_time = current_time
                    print("âš ï¸  é©¾é©¶å‘˜æ³¨æ„åŠ›è­¦å‘Š: æ£€æµ‹åˆ°é•¿æ—¶é—´é—­çœ¼!")
        else:
            if self.driver_attention_status == "Distracted":
                self.driver_attention_status = "Normal"
                print("âœ… é©¾é©¶å‘˜æ³¨æ„åŠ›æ¢å¤æ­£å¸¸")

    # =================== æ ¸å¿ƒå¤„ç†æµç¨‹ ===================

    def process_frame(self, frame):
        """
        å•å¸§å¤„ç†ä¸»æµç¨‹
        é›†æˆæ‰€æœ‰è¯†åˆ«åŠŸèƒ½
        """
        self.frame_count += 1
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # MediaPipe æ£€æµ‹
        hands_results = self.hands.process(rgb_frame)
        face_results = self.face_mesh.process(rgb_frame)

        # === æ‰‹åŠ¿è¯†åˆ«æµç¨‹ ===
        raw_gesture = self.detect_gesture(hands_results)
        stable_gesture = self.process_gesture_stable(raw_gesture)

        if stable_gesture != self.current_gesture:
            self.execute_gesture_command(stable_gesture)
            self.current_gesture = stable_gesture

        # === å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æµç¨‹ ===
        raw_head_action = self.detect_head_action(face_results)
        stable_head_action = self.process_head_action_stable(raw_head_action)

        if stable_head_action != self.current_head_action:
            self.execute_head_command(stable_head_action)
            self.current_head_action = stable_head_action

        # === çœ¼éƒ¨çŠ¶æ€ç›‘æ§æµç¨‹ ===
        eye_status = self.detect_eye_status(face_results)
        self.eyes_status = eye_status
        self.check_driver_attention(eye_status)

        # === ç»˜åˆ¶å¯è§†åŒ–ç•Œé¢ ===
        display_frame = self.draw_interface(frame, hands_results, face_results)

        return display_frame

    # =================== å¯è§†åŒ–ç•Œé¢ ===================

    def draw_interface(self, frame, hands_results, face_results):
        """ç»˜åˆ¶ç”¨æˆ·ç•Œé¢"""
        # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
        if hands_results.multi_hand_landmarks:
            for hand_landmarks in hands_results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(
                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style())

        # ç»˜åˆ¶çœ¼éƒ¨å…³é”®ç‚¹
        if face_results.multi_face_landmarks:
            for face_landmarks in face_results.multi_face_landmarks:
                # åªç»˜åˆ¶çœ¼éƒ¨å…³é”®ç‚¹
                for idx in self.face_landmarks_indices['left_eye'] + self.face_landmarks_indices['right_eye']:
                    landmark = face_landmarks.landmark[idx]
                    x = int(landmark.x * frame.shape[1])
                    y = int(landmark.y * frame.shape[0])
                    cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)

        height, width = frame.shape[:2]

        # åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (520, 300), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)

        font = cv2.FONT_HERSHEY_SIMPLEX

        # === çŠ¶æ€æ˜¾ç¤ºåŒºåŸŸ ===
        cv2.putText(frame, f"Hand: {self.current_gesture}", (20, 40),
                    font, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f"Head: {self.current_head_action}", (20, 70),
                    font, 0.6, (255, 255, 0), 2)

        # çœ¼éƒ¨çŠ¶æ€
        eye_color = (0, 255, 0) if self.eyes_status == "Open" else (0, 0, 255)
        cv2.putText(frame, f"Eyes: {self.eyes_status}", (20, 100),
                    font, 0.6, eye_color, 2)

        # æ³¨æ„åŠ›çŠ¶æ€
        attention_color = (0, 255, 0) if self.driver_attention_status == "Normal" else (0, 0, 255)
        cv2.putText(frame, f"Attention: {self.driver_attention_status}", (20, 130),
                    font, 0.6, attention_color, 2)

        # === åŠŸèƒ½è¯´æ˜åŒºåŸŸ ===
        y_offset = 160
        features = [
            "=== æ‰‹åŠ¿åŠŸèƒ½ ===",
            "å¼ å¼€æ‰‹æŒ â†’ æ’­æ”¾éŸ³ä¹",
            "æ¡æ‹³ â†’ æš‚åœéŸ³ä¹",
            "é£ŸæŒ‡å‘ä¸Š â†’ å‡æ¸©",
            "åŒæŒ‡å‘ä¸Š â†’ é™æ¸©",
            "",
            "=== å¤´éƒ¨åŠ¨ä½œ ===",
            "ç‚¹å¤´ â†’ ç¡®è®¤æ“ä½œ",
            "æ‘‡å¤´ â†’ å–æ¶ˆæ“ä½œ",
            "",
            "=== æŒ‰é”®è°ƒè¯• ===",
            "1/2: ç‚¹å¤´é˜ˆå€¼ Â±",
            "3/4: æ‘‡å¤´é˜ˆå€¼ Â±",
            "R: é‡ç½®å‚æ•°",
            "Q/ESC: é€€å‡º"
        ]

        for i, feature in enumerate(features):
            if feature == "":
                continue

            color = (255, 255, 255)
            if "===" in feature:
                color = (0, 255, 255)
            elif (self.current_gesture in feature) or (self.current_head_action in feature):
                color = (0, 255, 0)

            cv2.putText(frame, feature, (20, y_offset + i * 15),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

        # === ç³»ç»Ÿä¿¡æ¯åŒºåŸŸ ===
        # å¤´éƒ¨åŠ¨ä½œè°ƒè¯•ä¿¡æ¯
        if len(self.head_movement_history) > 0:
            current_pos = self.head_movement_history[-1]
            cv2.putText(frame, f"Head Pos: ({current_pos[0]:.3f}, {current_pos[1]:.3f})",
                        (20, height - 80), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

            if len(self.head_movement_history) >= 5:
                positions = list(self.head_movement_history)
                y_positions = [pos[1] for pos in positions]
                x_positions = [pos[0] for pos in positions]
                y_range = max(y_positions) - min(y_positions)
                x_range = max(x_positions) - min(x_positions)

                cv2.putText(frame, f"Y_range: {y_range:.4f} (éœ€è¦>{self.nod_threshold:.4f})",
                            (20, height - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.4,
                            (0, 255, 0) if y_range > self.nod_threshold else (255, 255, 255), 1)
                cv2.putText(frame, f"X_range: {x_range:.4f} (éœ€è¦>{self.head_movement_threshold:.4f})",
                            (20, height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.4,
                            (0, 255, 0) if x_range > self.head_movement_threshold else (255, 255, 255), 1)

        # é—­çœ¼è®¡æ•°å™¨
        if self.consecutive_closed_frames > 0:
            closed_seconds = self.consecutive_closed_frames / 30.0
            cv2.putText(frame, f"Closed: {closed_seconds:.1f}s", (350, 40),
                        font, 0.5, (0, 165, 255), 1)

        # æŒ‡ä»¤å†·å´çŠ¶æ€
        current_time = time.time()
        cooldown_remaining = max(0, self.command_cooldown - (current_time - self.last_command_time))
        if cooldown_remaining > 0:
            cv2.putText(frame, f"Gesture Cooldown: {cooldown_remaining:.1f}s", (280, height - 80),
                        font, 0.4, (255, 100, 100), 1)

        head_cooldown_remaining = max(0, self.command_cooldown - (current_time - self.last_head_command_time))
        if head_cooldown_remaining > 0:
            cv2.putText(frame, f"Head Cooldown: {head_cooldown_remaining:.1f}s", (280, height - 60),
                        font, 0.4, (255, 100, 100), 1)

        # å¸§æ•°è®¡æ•°
        cv2.putText(frame, f"Frame: {self.frame_count}", (450, height - 20),
                    font, 0.4, (255, 255, 0), 1)

        return frame

    # =================== ç³»ç»Ÿæ§åˆ¶ ===================

    def start_recognition(self, camera_index=0):
        """å¯åŠ¨è§†è§‰è¯†åˆ«ç³»ç»Ÿ"""
        print(f"\nğŸš€ å¯åŠ¨è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿ")

        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
            return

        # æ‘„åƒå¤´é…ç½®
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)

        cv2.namedWindow("è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«", cv2.WINDOW_NORMAL)
        self.is_running = True

        try:
            while self.is_running:
                ret, frame = cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    break

                # å¤„ç†å¸§
                processed_frame = self.process_frame(frame)
                cv2.imshow("è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«", processed_frame)

                # æŒ‰é”®æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC
                    break
                elif key == ord('q'):
                    break
                elif key == ord('1'):  # é™ä½ç‚¹å¤´é˜ˆå€¼
                    self.nod_threshold = max(0.005, self.nod_threshold - 0.005)
                    print(f"ç‚¹å¤´é˜ˆå€¼è°ƒæ•´ä¸º: {self.nod_threshold:.4f}")
                elif key == ord('2'):  # æé«˜ç‚¹å¤´é˜ˆå€¼
                    self.nod_threshold = min(0.050, self.nod_threshold + 0.005)
                    print(f"ç‚¹å¤´é˜ˆå€¼è°ƒæ•´ä¸º: {self.nod_threshold:.4f}")
                elif key == ord('3'):  # é™ä½æ‘‡å¤´é˜ˆå€¼
                    self.head_movement_threshold = max(0.010, self.head_movement_threshold - 0.005)
                    print(f"æ‘‡å¤´é˜ˆå€¼è°ƒæ•´ä¸º: {self.head_movement_threshold:.4f}")
                elif key == ord('4'):  # æé«˜æ‘‡å¤´é˜ˆå€¼
                    self.head_movement_threshold = min(0.050, self.head_movement_threshold + 0.005)
                    print(f"æ‘‡å¤´é˜ˆå€¼è°ƒæ•´ä¸º: {self.head_movement_threshold:.4f}")
                elif key == ord('r'):  # é‡ç½®å‚æ•°
                    self.nod_threshold = 0.015
                    self.head_movement_threshold = 0.025
                    print("å‚æ•°å·²é‡ç½®ä¸ºé»˜è®¤å€¼")

        except KeyboardInterrupt:
            print("ç”¨æˆ·ä¸­æ–­")
        finally:
            self.stop()

    def stop(self):
        """åœæ­¢è¯†åˆ«ç³»ç»Ÿ"""
        print("ğŸ›‘ åœæ­¢è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿ")
        self.is_running = False
        cv2.destroyAllWindows()

    # =================== å…¼å®¹æ€§æ¥å£ ===================

    def test_camera(self, camera_index: int = 0) -> bool:
        """æµ‹è¯•æ‘„åƒå¤´"""
        try:
            cap = cv2.VideoCapture(camera_index)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
                return False
            ret, frame = cap.read()
            cap.release()
            if ret:
                print(f"âœ… æ‘„åƒå¤´ {camera_index} æµ‹è¯•æˆåŠŸ")
                return True
            else:
                print(f"âŒ æ‘„åƒå¤´ {camera_index} æ— æ³•è¯»å–å¸§")
                return False
        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´æµ‹è¯•é”™è¯¯: {e}")
            return False

    def start_camera_recognition(self, camera_index: int = 0):
        """å¼€å§‹æ‘„åƒå¤´è¯†åˆ«ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰"""
        self.start_recognition(camera_index)

    def get_current_frame(self):
        """è·å–å½“å‰æ‘„åƒå¤´å¸§ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰"""
        return None


# =================== æµ‹è¯•å’Œæ¼”ç¤º ===================

def test_vision_system():
    """æµ‹è¯•è½¦è½½è§†è§‰è¯†åˆ«ç³»ç»Ÿ"""

    def command_callback(cmd_type, cmd_text):
        print(f"ğŸ¯ ç³»ç»Ÿæ¥æ”¶æŒ‡ä»¤: [{cmd_type}] {cmd_text}")
        # è¿™é‡Œå¯ä»¥è°ƒç”¨å®é™…çš„è½¦è½½ç³»ç»ŸAPI

    vision = EnhancedVisionRecognition(command_callback)

    try:
        vision.start_recognition()
    except KeyboardInterrupt:
        print("æµ‹è¯•ç»“æŸ")
    finally:
        vision.stop()


if __name__ == "__main__":
    test_vision_system()
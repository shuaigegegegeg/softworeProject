import os
import cv2
import time
import tempfile
from openai import OpenAI
from PIL import Image
import base64
import threading
from typing import Optional, Callable


class VisionRecognition:
    def __init__(self, command_callback: Callable[[str, str], None]):
        """
        åˆå§‹åŒ–è§†è§‰è¯†åˆ«æ¨¡å—

        Args:
            command_callback: å›è°ƒå‡½æ•°ï¼Œå‚æ•°ä¸º(command_type, command_text)
        """
        self.command_callback = command_callback

        # APIé…ç½®
        self.client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key="b220fba9-e27b-4e7c-bf61-5bc0ff995b70"
        )

        # è¯†åˆ«æ§åˆ¶
        self.is_running = False
        self.analysis_interval = 3  # åˆ†æé—´éš”ï¼ˆç§’ï¼‰

        # çŠ¶æ€è®°å½• - ç®€åŒ–ç‰ˆ
        self.current_state = "æ­£å¸¸"  # åªæœ‰"æ­£å¸¸"å’Œ"åˆ†å¿ƒ"ä¸¤ç§çŠ¶æ€
        self.eyes_closed_count = 0  # è¿ç»­é—­çœ¼å¸§æ•°
        self.last_gesture = "æ— "
        self.last_gesture_time = 0
        self.gesture_cooldown = 3  # æ‰‹åŠ¿å†·å´æ—¶é—´ï¼ˆç§’ï¼‰

        # å½“å‰å¸§ç¼“å­˜
        self.current_frame = None
        self.frame_lock = threading.Lock()
        self.camera_cap = None

        # æ‰‹åŠ¿æŒ‡ä»¤æ˜ å°„ - ç®€åŒ–ç‰ˆ
        self.gesture_commands = {
            'å¼ å¼€æ‰‹æŒ': 'æ’­æ”¾éŸ³ä¹',
            'æ¡æ‹³': 'æš‚åœéŸ³ä¹',
            'å¤§æ‹‡æŒ‡å‘ä¸Š': 'å‡æ¸©',
            'å¤§æ‹‡æŒ‡å‘ä¸‹': 'é™æ¸©'
        }

        print("ğŸ“¹ ç®€åŒ–ç‰ˆè§†è§‰è¯†åˆ«æ¨¡å—å·²åˆå§‹åŒ–")

    def get_current_frame(self):
        """è·å–å½“å‰æ‘„åƒå¤´å¸§"""
        with self.frame_lock:
            if self.current_frame is not None:
                return self.current_frame.copy()
            return None

    def create_driver_analysis_prompt(self) -> str:
        """åˆ›å»ºé©¾é©¶å‘˜çŠ¶æ€åˆ†ææç¤ºè¯ - ç®€åŒ–ç‰ˆ"""
        return """è¯·åˆ†æå›¾ç‰‡ä¸­é©¾é©¶å‘˜çš„çŠ¶æ€ï¼Œåªåˆ¤æ–­æ˜¯å¦åˆ†å¿ƒï¼š

**åˆ†å¿ƒåˆ¤æ–­æ ‡å‡†ï¼š**
1. å¤´éƒ¨æ˜æ˜¾å‘ä¸‹ä½ç€ï¼ˆè¶…è¿‡30åº¦ï¼‰
2. å¤´éƒ¨æ˜æ˜¾å‘ä¸Šä»°ç€ï¼ˆè¶…è¿‡30åº¦ï¼‰  
3. åŒçœ¼å®Œå…¨é—­åˆ

**æ­£å¸¸åˆ¤æ–­æ ‡å‡†ï¼š**
1. å¤´éƒ¨ä¿æŒæ­£å¸¸å‰è§†å§¿åŠ¿
2. çœ¼ç›çå¼€

è¯·ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
çŠ¶æ€ï¼š[æ­£å¸¸/åˆ†å¿ƒ]
åŸå› ï¼šç®€è¦è¯´æ˜åˆ¤æ–­ä¾æ®

åªå›ç­”è¿™ä¸¤é¡¹ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    def create_gesture_prompt(self) -> str:
        """åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«æç¤ºè¯ - ç®€åŒ–ç‰ˆ"""
        return """è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰‹åŠ¿ï¼Œåªè¯†åˆ«ä»¥ä¸‹4ç§ï¼š

1. **å¼ å¼€æ‰‹æŒ**ï¼šäº”æŒ‡ä¼¸ç›´åˆ†å¼€ï¼Œæ‰‹æŒå±•å¼€
2. **æ¡æ‹³**ï¼šäº”æŒ‡ç´§æ¡æˆæ‹³å¤´
3. **å¤§æ‹‡æŒ‡å‘ä¸Š**ï¼šæ‹‡æŒ‡ç«–èµ·æŒ‡å‘ä¸Šæ–¹
4. **å¤§æ‹‡æŒ‡å‘ä¸‹**ï¼šæ‹‡æŒ‡æŒ‡å‘ä¸‹æ–¹

è¯·ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ‰‹åŠ¿ï¼š[å¼ å¼€æ‰‹æŒ/æ¡æ‹³/å¤§æ‹‡æŒ‡å‘ä¸Š/å¤§æ‹‡æŒ‡å‘ä¸‹/æ— ]

å¦‚æœä¸æ˜¯ä¸Šè¿°4ç§æ‰‹åŠ¿ï¼Œè¯·å›ç­”"æ— "ã€‚"""

    def analyze_image(self, image_path: str, analysis_type: str) -> Optional[str]:
        """åˆ†æå›¾åƒ"""
        try:
            with open(image_path, "rb") as image_file:
                encoded_image = base64.b64encode(image_file.read()).decode('utf-8')

            if analysis_type == "driver":
                prompt = self.create_driver_analysis_prompt()
            elif analysis_type == "gesture":
                prompt = self.create_gesture_prompt()
            else:
                return None

            response = self.client.chat.completions.create(
                model="doubao-1.5-vision-pro-250328",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{encoded_image}"
                                },
                            },
                            {"type": "text", "text": prompt},
                        ],
                    }
                ],
                max_tokens=300
            )

            return response.choices[0].message.content

        except Exception as e:
            print(f"âŒ è§†è§‰åˆ†æé”™è¯¯: {e}")
            return None

    def parse_driver_state(self, analysis_result: str) -> Optional[str]:
        """è§£æé©¾é©¶å‘˜çŠ¶æ€ - ç®€åŒ–ç‰ˆ"""
        if not analysis_result:
            return None

        analysis_lower = analysis_result.lower()

        # æ£€æŸ¥æ˜¯å¦æåˆ°åˆ†å¿ƒçš„å…³é”®è¯
        distraction_keywords = ['åˆ†å¿ƒ', 'ä½å¤´', 'ä»°å¤´', 'é—­çœ¼', 'é—­åˆ']
        if any(keyword in analysis_lower for keyword in distraction_keywords):
            return "åˆ†å¿ƒ"

        # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯´æ­£å¸¸
        if 'æ­£å¸¸' in analysis_lower:
            return "æ­£å¸¸"

        # å°è¯•ä»æ ¼å¼åŒ–è¾“å‡ºä¸­æå–
        if 'çŠ¶æ€ï¼š' in analysis_result:
            try:
                for line in analysis_result.split('\n'):
                    if 'çŠ¶æ€ï¼š' in line:
                        state = line.split('çŠ¶æ€ï¼š')[1].strip()
                        if 'åˆ†å¿ƒ' in state:
                            return "åˆ†å¿ƒ"
                        elif 'æ­£å¸¸' in state:
                            return "æ­£å¸¸"
            except:
                pass

        return "æ­£å¸¸"  # é»˜è®¤è¿”å›æ­£å¸¸

    def parse_gesture(self, analysis_result: str) -> Optional[str]:
        """è§£ææ‰‹åŠ¿ - ç®€åŒ–ç‰ˆ"""
        if not analysis_result:
            return "æ— "

        analysis_lower = analysis_result.lower()

        # ç›´æ¥æ£€æŸ¥å…³é”®è¯
        if 'å¼ å¼€æ‰‹æŒ' in analysis_lower or 'æ‰‹æŒå¼ å¼€' in analysis_lower:
            return 'å¼ å¼€æ‰‹æŒ'
        elif 'æ¡æ‹³' in analysis_lower or 'æ‹³å¤´' in analysis_lower:
            return 'æ¡æ‹³'
        elif 'æ‹‡æŒ‡å‘ä¸Š' in analysis_lower or 'å¤§æ‹‡æŒ‡å‘ä¸Š' in analysis_lower:
            return 'å¤§æ‹‡æŒ‡å‘ä¸Š'
        elif 'æ‹‡æŒ‡å‘ä¸‹' in analysis_lower or 'å¤§æ‹‡æŒ‡å‘ä¸‹' in analysis_lower:
            return 'å¤§æ‹‡æŒ‡å‘ä¸‹'

        # å°è¯•ä»æ ¼å¼åŒ–è¾“å‡ºä¸­æå–
        if 'æ‰‹åŠ¿ï¼š' in analysis_result:
            try:
                for line in analysis_result.split('\n'):
                    if 'æ‰‹åŠ¿ï¼š' in line:
                        gesture = line.split('æ‰‹åŠ¿ï¼š')[1].strip()
                        for cmd_gesture in self.gesture_commands.keys():
                            if cmd_gesture in gesture:
                                return cmd_gesture
            except:
                pass

        return "æ— "

    def process_driver_state(self, state: str):
        """å¤„ç†é©¾é©¶å‘˜çŠ¶æ€ - ç®€åŒ–ç‰ˆ"""
        if state != self.current_state:
            print(f"ğŸ“Š é©¾é©¶å‘˜çŠ¶æ€: {self.current_state} -> {state}")
            self.current_state = state

            # å‘é€çŠ¶æ€æ›´æ–°
            try:
                self.command_callback('driver_state', state)
            except Exception as e:
                print(f"âŒ çŠ¶æ€å›è°ƒé”™è¯¯: {e}")

            # åˆ†å¿ƒæ—¶å‘é€è¯­éŸ³æé†’
            if state == "åˆ†å¿ƒ":
                print("âš ï¸ æ£€æµ‹åˆ°é©¾é©¶å‘˜åˆ†å¿ƒï¼Œå‘é€è¯­éŸ³æé†’")
                try:
                    self.command_callback('voice_warning', 'è¯·é›†ä¸­ç²¾ç¥æ³¨æ„è·¯å†µ')
                except Exception as e:
                    print(f"âŒ è¯­éŸ³æé†’å›è°ƒé”™è¯¯: {e}")

    def process_gesture(self, gesture: str):
        """å¤„ç†æ‰‹åŠ¿ - ç®€åŒ–ç‰ˆ"""
        current_time = time.time()

        # æ‰‹åŠ¿å»é‡å’Œå†·å´
        if gesture == self.last_gesture or gesture == "æ— ":
            return

        if current_time - self.last_gesture_time < self.gesture_cooldown:
            print(f"â±ï¸ æ‰‹åŠ¿å†·å´ä¸­ï¼Œå¿½ç•¥: {gesture}")
            return

        print(f"ğŸ‘‹ è¯†åˆ«åˆ°æ‰‹åŠ¿: {gesture}")
        self.last_gesture = gesture
        self.last_gesture_time = current_time

        # å‘é€æ‰‹åŠ¿çŠ¶æ€æ›´æ–°
        try:
            self.command_callback('gesture', gesture)
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¿çŠ¶æ€å›è°ƒé”™è¯¯: {e}")

        # æ‰§è¡Œæ‰‹åŠ¿æŒ‡ä»¤
        if gesture in self.gesture_commands:
            command = self.gesture_commands[gesture]
            print(f"âœ… æ‰§è¡Œæ‰‹åŠ¿æŒ‡ä»¤: {gesture} -> {command}")

            try:
                # å‘é€æŒ‡ä»¤ï¼ˆä½¿ç”¨voiceç±»å‹ï¼Œè¿™æ ·main.pyèƒ½æ­£ç¡®å¤„ç†ï¼‰
                self.command_callback('voice', command)
                print(f"ğŸ“¤ æ‰‹åŠ¿æŒ‡ä»¤å·²å‘é€: {command}")
            except Exception as e:
                print(f"âŒ æ‰‹åŠ¿æŒ‡ä»¤å›è°ƒé”™è¯¯: {e}")

    def analyze_camera_frame(self, frame):
        """åˆ†ææ‘„åƒå¤´å¸§ - ç®€åŒ–ç‰ˆ"""
        try:
            # è½¬æ¢ä¸ºPILæ ¼å¼
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(rgb_frame)

            # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as f:
                temp_path = f.name
                pil_img.save(temp_path, quality=85)

            # åˆ†æé©¾é©¶å‘˜çŠ¶æ€
            driver_analysis = self.analyze_image(temp_path, "driver")
            if driver_analysis:
                driver_state = self.parse_driver_state(driver_analysis)
                if driver_state:
                    # å¤„ç†è¿ç»­é—­çœ¼æ£€æµ‹
                    if 'é—­çœ¼' in driver_analysis.lower() or 'é—­åˆ' in driver_analysis.lower():
                        self.eyes_closed_count += 1
                        print(f"ğŸ‘ï¸ æ£€æµ‹åˆ°é—­çœ¼ï¼Œè¿ç»­æ¬¡æ•°: {self.eyes_closed_count}")
                        if self.eyes_closed_count >= 2:  # è¿ç»­ä¸¤æ¬¡é—­çœ¼
                            print("âš ï¸ è¿ç»­é—­çœ¼è¶…è¿‡2æ¬¡ï¼Œåˆ¤å®šä¸ºåˆ†å¿ƒ")
                            driver_state = "åˆ†å¿ƒ"
                            self.eyes_closed_count = 0  # é‡ç½®
                    else:
                        self.eyes_closed_count = 0  # é‡ç½®é—­çœ¼è®¡æ•°

                    self.process_driver_state(driver_state)

            # åˆ†ææ‰‹åŠ¿
            gesture_analysis = self.analyze_image(temp_path, "gesture")
            if gesture_analysis:
                gesture = self.parse_gesture(gesture_analysis)
                if gesture:
                    self.process_gesture(gesture)

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)

        except Exception as e:
            print(f"âŒ å¸§åˆ†æé”™è¯¯: {e}")

    def start_camera_recognition(self, camera_index: int = 0):
        """å¼€å§‹æ‘„åƒå¤´è¯†åˆ«"""
        print(f"ğŸ“¹ å¯åŠ¨æ‘„åƒå¤´è§†è§‰è¯†åˆ«...")
        self.is_running = True

        while self.is_running:
            try:
                self.camera_cap = cv2.VideoCapture(camera_index)
                if not self.camera_cap.isOpened():
                    print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
                    time.sleep(5)
                    continue

                print(f"âœ… æ‘„åƒå¤´ {camera_index} å·²å¯åŠ¨")
                self.camera_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                self.camera_cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

                last_analysis_time = 0
                frame_count = 0

                while self.is_running:
                    ret, frame = self.camera_cap.read()
                    if not ret:
                        print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                        break

                    frame_count += 1
                    current_time = time.time()

                    # æ›´æ–°å½“å‰å¸§
                    with self.frame_lock:
                        self.current_frame = frame.copy()

                    # æŒ‰é—´éš”åˆ†æå¸§
                    if current_time - last_analysis_time >= self.analysis_interval:
                        print(f"ğŸ” åˆ†æç¬¬ {frame_count} å¸§...")
                        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­åˆ†æ
                        analysis_thread = threading.Thread(
                            target=self.analyze_camera_frame,
                            args=(frame.copy(),),
                            daemon=True
                        )
                        analysis_thread.start()
                        last_analysis_time = current_time

                    time.sleep(0.066)  # çº¦15fps

            except Exception as e:
                print(f"âŒ æ‘„åƒå¤´è¿è¡Œé”™è¯¯: {e}")
                time.sleep(5)
            finally:
                if self.camera_cap:
                    self.camera_cap.release()
                    self.camera_cap = None

    def test_camera(self, camera_index: int = 0) -> bool:
        """æµ‹è¯•æ‘„åƒå¤´"""
        try:
            cap = cv2.VideoCapture(camera_index)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
                return False
            ret, frame = cap.read()
            cap.release()
            if ret:
                print(f"âœ… æ‘„åƒå¤´ {camera_index} æµ‹è¯•æˆåŠŸ")
                return True
            else:
                print(f"âŒ æ‘„åƒå¤´ {camera_index} æ— æ³•è¯»å–å¸§")
                return False
        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´æµ‹è¯•é”™è¯¯: {e}")
            return False

    def stop(self):
        """åœæ­¢è§†è§‰è¯†åˆ«"""
        print("ğŸ›‘ åœæ­¢è§†è§‰è¯†åˆ«...")
        self.is_running = False
        if self.camera_cap:
            self.camera_cap.release()
            self.camera_cap = None
        with self.frame_lock:
            self.current_frame = None
        print("âœ… è§†è§‰è¯†åˆ«å·²åœæ­¢")


# æµ‹è¯•å‡½æ•°
def test_simple_vision():
    """æµ‹è¯•ç®€åŒ–ç‰ˆè§†è§‰è¯†åˆ«"""

    def command_callback(cmd_type, cmd_text):
        print(f"æ”¶åˆ°æŒ‡ä»¤: [{cmd_type}] {cmd_text}")

    vision = VisionRecognition(command_callback)

    if not vision.test_camera():
        print("æ‘„åƒå¤´æµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
        return

    try:
        print("å¼€å§‹ç®€åŒ–ç‰ˆè§†è§‰è¯†åˆ«æµ‹è¯•ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
        vision.start_camera_recognition()
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    finally:
        vision.stop()


if __name__ == "__main__":
    test_simple_vision()
import cv2
import time
import numpy as np
from collections import deque
import mediapipe as mp
from typing import Optional, Callable
import threading
import math


class VisionRecognition:
    """
    è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«æ¨¡å— - å…¼å®¹main.pyçš„é›†æˆç‰ˆæœ¬
    é›†æˆåŠŸèƒ½ï¼š
    1. æ‰‹åŠ¿è¯†åˆ« - æ§åˆ¶éŸ³ä¹ã€ç©ºè°ƒç­‰
    2. å¤´éƒ¨åŠ¨ä½œè¯†åˆ« - ç¡®è®¤/å–æ¶ˆæ“ä½œ
    3. çœ¼éƒ¨çŠ¶æ€ç›‘æ§ - é©¾é©¶å‘˜æ³¨æ„åŠ›æ£€æµ‹
    """

    def __init__(self, command_callback: Optional[Callable[[str, str], None]] = None):
        self.command_callback = command_callback or self.default_callback

        # ===== é›†æˆå…¼å®¹æ€§å±æ€§ =====
        self.is_running = False
        self.camera_cap = None
        self.current_frame = None
        self.vision_thread = None
        self.should_stop = False

        # ===== MediaPipe åˆå§‹åŒ– =====
        self.mp_hands = mp.solutions.hands
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        # æ‰‹éƒ¨æ£€æµ‹å™¨
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )

        # é¢éƒ¨æ£€æµ‹å™¨
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )

        # ===== æ‰‹åŠ¿è¯†åˆ«å‚æ•° =====
        self.finger_threshold = 0.02
        self.gesture_stability_frames = 5
        self.gesture_history = deque(maxlen=self.gesture_stability_frames)
        self.current_gesture = "None"

        # ===== å¤´éƒ¨åŠ¨ä½œè¯†åˆ«å‚æ•° =====
        self.head_movement_threshold = 0.1
        self.nod_threshold = 0.1
        self.head_action_frames = 10
        self.head_movement_history = deque(maxlen=20)
        self.head_action_history = deque(maxlen=self.head_action_frames)
        self.current_head_action = "None"

        # ===== çœ¼éƒ¨çŠ¶æ€ç›‘æ§å‚æ•° =====
        self.eye_aspect_ratio_threshold = 0.25
        self.eye_closed_frames_threshold = 60
        self.consecutive_closed_frames = 0
        self.eyes_status = "Open"
        self.driver_attention_status = "Normal"
        self.ear_history = deque(maxlen=10)

        # ===== æŒ‡ä»¤æ§åˆ¶å‚æ•° =====
        self.command_cooldown = 2.0
        self.last_command_time = 0
        self.last_head_command_time = 0
        self.last_attention_alert_time = 0

        # ===== ç³»ç»ŸçŠ¶æ€ =====
        self.frame_count = 0

        # ===== æŒ‡ä»¤æ˜ å°„é…ç½® =====
        self.gesture_commands = {
            'Open Palm': 'æ’­æ”¾éŸ³ä¹',
            'Fist': 'æš‚åœéŸ³ä¹',
            'Index Up': 'å‡æ¸©',
            'Two Fingers Up': 'é™æ¸©'
        }

        self.head_action_commands = {
            'Nod': 'ç¡®è®¤æ“ä½œ',
            'Shake': 'å–æ¶ˆæ“ä½œ'
        }

        # ===== é¢éƒ¨å…³é”®ç‚¹ç´¢å¼• =====
        self.face_landmarks_indices = {
            'nose_tip': 1,
            'left_eye': {
                'outer_corner': 33,
                'inner_corner': 133,
                'top_1': 159,
                'top_2': 158,
                'bottom_1': 145,
                'bottom_2': 153
            },
            'right_eye': {
                'outer_corner': 362,
                'inner_corner': 263,
                'top_1': 386,
                'top_2': 385,
                'bottom_1': 374,
                'bottom_2': 380
            }
        }

        print("ğŸ¯ è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿå·²åˆå§‹åŒ–")

    def default_callback(self, cmd_type: str, cmd_text: str):
        """é»˜è®¤å›è°ƒå‡½æ•°"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] ğŸ¯ {cmd_type}: {cmd_text}")

    def get_current_frame(self):
        """è·å–å½“å‰å¸§ - å…¼å®¹æ¥å£"""
        return self.current_frame

    def test_camera(self, camera_index: int = 0) -> bool:
        """æµ‹è¯•æ‘„åƒå¤´ - å…¼å®¹æ¥å£"""
        try:
            cap = cv2.VideoCapture(camera_index)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
                return False
            ret, frame = cap.read()
            cap.release()
            if ret:
                print(f"âœ… æ‘„åƒå¤´ {camera_index} æµ‹è¯•æˆåŠŸ")
                return True
            else:
                print(f"âŒ æ‘„åƒå¤´ {camera_index} æ— æ³•è¯»å–å¸§")
                return False
        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´æµ‹è¯•é”™è¯¯: {e}")
            return False

    # =================== æ‰‹åŠ¿è¯†åˆ«æ¨¡å— ===================

    def detect_gesture(self, hands_results):
        """æ‰‹åŠ¿è¯†åˆ«æ ¸å¿ƒç®—æ³•"""
        if not hands_results.multi_hand_landmarks:
            return "None"

        landmarks = hands_results.multi_hand_landmarks[0].landmark

        # è·å–æ‰‹æŒ‡å…³é”®ç‚¹
        finger_landmarks = {
            'thumb': {'tip': landmarks[4], 'pip': landmarks[3]},
            'index': {'tip': landmarks[8], 'pip': landmarks[6]},
            'middle': {'tip': landmarks[12], 'pip': landmarks[10]},
            'ring': {'tip': landmarks[16], 'pip': landmarks[14]},
            'pinky': {'tip': landmarks[20], 'pip': landmarks[18]}
        }

        # åˆ¤æ–­æ¯ä¸ªæ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´
        fingers_extended = []

        # æ‹‡æŒ‡ç‰¹æ®Šå¤„ç†ï¼ˆæ°´å¹³ä¼¸å±•ï¼‰
        thumb_extended = abs(finger_landmarks['thumb']['tip'].x -
                             finger_landmarks['thumb']['pip'].x) > self.finger_threshold
        fingers_extended.append(thumb_extended)

        # å…¶ä»–å››æŒ‡ï¼ˆå‚ç›´ä¼¸å±•ï¼‰
        for finger_name in ['index', 'middle', 'ring', 'pinky']:
            tip = finger_landmarks[finger_name]['tip']
            pip = finger_landmarks[finger_name]['pip']
            extended = tip.y < pip.y - self.finger_threshold
            fingers_extended.append(extended)

        # æ‰‹åŠ¿è¯†åˆ«é€»è¾‘
        extended_count = sum(fingers_extended)

        if extended_count >= 4:
            return "Open Palm"
        elif extended_count <= 1:
            return "Fist"
        elif (fingers_extended[1] and fingers_extended[2] and
              not fingers_extended[3] and not fingers_extended[4]):
            return "Two Fingers Up"
        elif (fingers_extended[1] and not fingers_extended[2] and
              not fingers_extended[3] and not fingers_extended[4]):
            return "Index Up"
        else:
            return "None"

    def process_gesture_stable(self, raw_gesture):
        """æ‰‹åŠ¿ç¨³å®šæ€§å¤„ç†"""
        self.gesture_history.append(raw_gesture)

        if len(self.gesture_history) < self.gesture_stability_frames:
            return self.current_gesture

        # ç»Ÿè®¡æœ€è¿‘æ‰‹åŠ¿
        recent_gestures = list(self.gesture_history)
        gesture_counts = {}
        for g in recent_gestures:
            gesture_counts[g] = gesture_counts.get(g, 0) + 1

        if gesture_counts:
            most_common = max(gesture_counts, key=gesture_counts.get)
            required_count = self.gesture_stability_frames // 2 + 1
            if gesture_counts[most_common] >= required_count:
                return most_common

        return self.current_gesture

    def execute_gesture_command(self, gesture):
        """æ‰§è¡Œæ‰‹åŠ¿æŒ‡ä»¤"""
        current_time = time.time()

        if (gesture != "None" and
                gesture != self.current_gesture and
                current_time - self.last_command_time > self.command_cooldown):

            if gesture in self.gesture_commands:
                command = self.gesture_commands[gesture]
                self.command_callback('æ‰‹åŠ¿', command)
                self.last_command_time = current_time
                print(f"âœ… æ‰‹åŠ¿æŒ‡ä»¤: {gesture} â†’ {command}")

    # =================== å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æ¨¡å— ===================

    def detect_head_action(self, face_results):
        """å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æ ¸å¿ƒç®—æ³•"""
        if not face_results.multi_face_landmarks:
            return "None"

        face_landmarks = face_results.multi_face_landmarks[0]
        landmarks = face_landmarks.landmark

        try:
            # ä½¿ç”¨é¼»å°–ä½œä¸ºå¤´éƒ¨ä½ç½®å‚è€ƒç‚¹
            nose_tip = landmarks[self.face_landmarks_indices['nose_tip']]
            current_position = (nose_tip.x, nose_tip.y)

            self.head_movement_history.append(current_position)

            if len(self.head_movement_history) < self.head_action_frames:
                return "None"

            # åˆ†æå¤´éƒ¨ç§»åŠ¨æ¨¡å¼
            positions = list(self.head_movement_history)

            # Yè½´å˜åŒ–åˆ†æï¼ˆç‚¹å¤´ï¼‰
            y_positions = [pos[1] for pos in positions]
            y_range = max(y_positions) - min(y_positions)

            # Xè½´å˜åŒ–åˆ†æï¼ˆæ‘‡å¤´ï¼‰
            x_positions = [pos[0] for pos in positions]
            x_range = max(x_positions) - min(x_positions)

            # ç‚¹å¤´æ£€æµ‹
            if y_range > self.nod_threshold:
                max_y_idx = y_positions.index(max(y_positions))
                if 3 <= max_y_idx <= len(y_positions) - 4:
                    start_y = y_positions[0]
                    end_y = y_positions[-1]
                    max_y = y_positions[max_y_idx]

                    if (max_y > start_y + self.nod_threshold * 0.6 and
                            max_y > end_y + self.nod_threshold * 0.6):
                        return "Nod"

            # æ‘‡å¤´æ£€æµ‹
            if x_range > self.head_movement_threshold:
                direction_changes = 0
                for i in range(1, len(x_positions) - 1):
                    if ((x_positions[i] > x_positions[i - 1] and x_positions[i] > x_positions[i + 1]) or
                            (x_positions[i] < x_positions[i - 1] and x_positions[i] < x_positions[i + 1])):
                        direction_changes += 1

                if direction_changes >= 2:
                    return "Shake"

            return "None"

        except (IndexError, AttributeError):
            return "None"

    def process_head_action_stable(self, raw_action):
        """å¤´éƒ¨åŠ¨ä½œç¨³å®šæ€§å¤„ç†"""
        self.head_action_history.append(raw_action)

        if len(self.head_action_history) < self.head_action_frames // 2:
            return self.current_head_action

        recent_actions = list(self.head_action_history)
        action_counts = {}
        for a in recent_actions:
            action_counts[a] = action_counts.get(a, 0) + 1

        if action_counts:
            most_common = max(action_counts, key=action_counts.get)
            required_count = len(recent_actions) // 3 + 1
            if action_counts[most_common] >= required_count and most_common != "None":
                return most_common

        return "None"

    def execute_head_command(self, action):
        """æ‰§è¡Œå¤´éƒ¨åŠ¨ä½œæŒ‡ä»¤"""
        current_time = time.time()

        if (action != "None" and
                action != self.current_head_action and
                current_time - self.last_head_command_time > self.command_cooldown):

            if action in self.head_action_commands:
                command = self.head_action_commands[action]
                self.command_callback('å¤´éƒ¨åŠ¨ä½œ', command)
                self.last_head_command_time = current_time
                print(f"âœ… å¤´éƒ¨åŠ¨ä½œ: {action} â†’ {command}")

    # =================== çœ¼éƒ¨çŠ¶æ€ç›‘æ§æ¨¡å— ===================

    def calculate_eye_aspect_ratio(self, eye_landmarks):
        """è®¡ç®—çœ¼ç›å®½é«˜æ¯” (Eye Aspect Ratio - EAR)"""
        try:
            # è·å–çœ¼éƒ¨6ä¸ªå…³é”®ç‚¹çš„åæ ‡
            points = []
            for landmark in eye_landmarks:
                points.append([landmark.x, landmark.y])

            points = np.array(points)

            # è®¡ç®—å‚ç›´è·ç¦»
            vertical_1 = np.linalg.norm(points[1] - points[5])
            vertical_2 = np.linalg.norm(points[2] - points[4])

            # è®¡ç®—æ°´å¹³è·ç¦»
            horizontal = np.linalg.norm(points[0] - points[3])

            # EAR = (vertical_1 + vertical_2) / (2.0 * horizontal)
            if horizontal > 0:
                ear = (vertical_1 + vertical_2) / (2.0 * horizontal)
            else:
                ear = 0.0

            return ear

        except Exception as e:
            print(f"EARè®¡ç®—é”™è¯¯: {e}")
            return 0.3

    def detect_eye_status(self, face_results):
        """çœ¼éƒ¨çŠ¶æ€æ£€æµ‹"""
        if not face_results.multi_face_landmarks:
            return "Unknown"

        face_landmarks = face_results.multi_face_landmarks[0]
        landmarks = face_landmarks.landmark

        try:
            # è·å–å·¦çœ¼å…³é”®ç‚¹
            left_eye_indices = [
                self.face_landmarks_indices['left_eye']['outer_corner'],
                self.face_landmarks_indices['left_eye']['top_1'],
                self.face_landmarks_indices['left_eye']['top_2'],
                self.face_landmarks_indices['left_eye']['inner_corner'],
                self.face_landmarks_indices['left_eye']['bottom_1'],
                self.face_landmarks_indices['left_eye']['bottom_2']
            ]

            # è·å–å³çœ¼å…³é”®ç‚¹
            right_eye_indices = [
                self.face_landmarks_indices['right_eye']['outer_corner'],
                self.face_landmarks_indices['right_eye']['top_1'],
                self.face_landmarks_indices['right_eye']['top_2'],
                self.face_landmarks_indices['right_eye']['inner_corner'],
                self.face_landmarks_indices['right_eye']['bottom_1'],
                self.face_landmarks_indices['right_eye']['bottom_2']
            ]

            # è·å–å®é™…çš„å…³é”®ç‚¹åæ ‡
            left_eye_points = [landmarks[i] for i in left_eye_indices]
            right_eye_points = [landmarks[i] for i in right_eye_indices]

            # è®¡ç®—åŒçœ¼EAR
            left_ear = self.calculate_eye_aspect_ratio(left_eye_points)
            right_ear = self.calculate_eye_aspect_ratio(right_eye_points)
            avg_ear = (left_ear + right_ear) / 2.0

            # å°†EARå€¼æ·»åŠ åˆ°å†å²è®°å½•ä¸­è¿›è¡Œå¹³æ»‘å¤„ç†
            self.ear_history.append(avg_ear)

            # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥å¹³æ»‘EARå€¼
            if len(self.ear_history) > 0:
                smooth_ear = sum(self.ear_history) / len(self.ear_history)
            else:
                smooth_ear = avg_ear

            # çœ¼éƒ¨çŠ¶æ€åˆ¤æ–­
            if smooth_ear < self.eye_aspect_ratio_threshold:
                self.consecutive_closed_frames += 1
                if self.consecutive_closed_frames >= self.eye_closed_frames_threshold:
                    return "Closed_Long"
                else:
                    return "Closed"
            else:
                self.consecutive_closed_frames = 0
                return "Open"

        except (IndexError, AttributeError) as e:
            print(f"çœ¼éƒ¨æ£€æµ‹é”™è¯¯: {e}")
            return "Unknown"

    def check_driver_attention(self, eye_status):
        """é©¾é©¶å‘˜æ³¨æ„åŠ›çŠ¶æ€æ£€æŸ¥"""
        current_time = time.time()

        if eye_status == "Closed_Long":
            if self.driver_attention_status != "Distracted":
                self.driver_attention_status = "Distracted"
                # é˜²æ­¢é¢‘ç¹è­¦å‘Š
                if current_time - self.last_attention_alert_time > 5.0:
                    # å‘é€å¼€å§‹åˆ†å¿ƒè­¦å‘Šçš„æŒ‡ä»¤
                    self.command_callback('driver_distraction_start', 'æ£€æµ‹åˆ°é©¾é©¶å‘˜åˆ†å¿ƒ - é•¿æ—¶é—´é—­çœ¼')
                    self.last_attention_alert_time = current_time
                    print("âš ï¸  é©¾é©¶å‘˜æ³¨æ„åŠ›è­¦å‘Š: æ£€æµ‹åˆ°é•¿æ—¶é—´é—­çœ¼!")
        else:
            if self.driver_attention_status == "Distracted":
                self.driver_attention_status = "Normal"
                # å‘é€åœæ­¢åˆ†å¿ƒè­¦å‘Šçš„æŒ‡ä»¤
                self.command_callback('driver_distraction_end', 'é©¾é©¶å‘˜æ³¨æ„åŠ›æ¢å¤æ­£å¸¸')
                print("âœ… é©¾é©¶å‘˜æ³¨æ„åŠ›æ¢å¤æ­£å¸¸")

    # =================== æ ¸å¿ƒå¤„ç†æµç¨‹ ===================

    def process_frame(self, frame):
        """å•å¸§å¤„ç†ä¸»æµç¨‹"""
        if frame is None:
            return None

        self.frame_count += 1
        self.current_frame = frame.copy()  # æ›´æ–°å½“å‰å¸§

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # MediaPipe æ£€æµ‹
        hands_results = self.hands.process(rgb_frame)
        face_results = self.face_mesh.process(rgb_frame)

        # === æ‰‹åŠ¿è¯†åˆ«æµç¨‹ ===
        raw_gesture = self.detect_gesture(hands_results)
        stable_gesture = self.process_gesture_stable(raw_gesture)

        if stable_gesture != self.current_gesture:
            self.execute_gesture_command(stable_gesture)
            self.current_gesture = stable_gesture

        # === å¤´éƒ¨åŠ¨ä½œè¯†åˆ«æµç¨‹ ===
        raw_head_action = self.detect_head_action(face_results)
        stable_head_action = self.process_head_action_stable(raw_head_action)

        if stable_head_action != self.current_head_action:
            self.execute_head_command(stable_head_action)
            self.current_head_action = stable_head_action

        # === çœ¼éƒ¨çŠ¶æ€ç›‘æ§æµç¨‹ ===
        eye_status = self.detect_eye_status(face_results)
        self.eyes_status = eye_status
        self.check_driver_attention(eye_status)

        # === ç»˜åˆ¶å¯è§†åŒ–ç•Œé¢ ===
        display_frame = self.draw_interface(frame, hands_results, face_results)

        return display_frame

    def draw_interface(self, frame, hands_results, face_results):
        """ç»˜åˆ¶ç”¨æˆ·ç•Œé¢"""
        if frame is None:
            return None

        # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
        if hands_results.multi_hand_landmarks:
            for hand_landmarks in hands_results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(
                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style())

        # ç»˜åˆ¶çœ¼éƒ¨å…³é”®ç‚¹
        if face_results.multi_face_landmarks:
            for face_landmarks in face_results.multi_face_landmarks:
                landmarks = face_landmarks.landmark

                # ç»˜åˆ¶å·¦çœ¼å…³é”®ç‚¹
                for key, idx in self.face_landmarks_indices['left_eye'].items():
                    landmark = landmarks[idx]
                    x = int(landmark.x * frame.shape[1])
                    y = int(landmark.y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)

                # ç»˜åˆ¶å³çœ¼å…³é”®ç‚¹
                for key, idx in self.face_landmarks_indices['right_eye'].items():
                    landmark = landmarks[idx]
                    x = int(landmark.x * frame.shape[1])
                    y = int(landmark.y * frame.shape[0])
                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)

        height, width = frame.shape[:2]

        # åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)

        font = cv2.FONT_HERSHEY_SIMPLEX

        # === çŠ¶æ€æ˜¾ç¤ºåŒºåŸŸ ===
        cv2.putText(frame, f"Hand: {self.current_gesture}", (20, 40),
                    font, 0.6, (0, 255, 0), 2)
        cv2.putText(frame, f"Head: {self.current_head_action}", (20, 70),
                    font, 0.6, (255, 255, 0), 2)

        # çœ¼éƒ¨çŠ¶æ€
        eye_color = (0, 255, 0) if self.eyes_status == "Open" else (0, 0, 255)
        cv2.putText(frame, f"Eyes: {self.eyes_status}", (20, 100),
                    font, 0.6, eye_color, 2)

        # æ³¨æ„åŠ›çŠ¶æ€
        attention_color = (0, 255, 0) if self.driver_attention_status == "Normal" else (0, 0, 255)
        cv2.putText(frame, f"Attention: {self.driver_attention_status}", (20, 130),
                    font, 0.6, attention_color, 2)

        # EARå€¼æ˜¾ç¤º
        if len(self.ear_history) > 0:
            current_ear = self.ear_history[-1]
            cv2.putText(frame, f"EAR: {current_ear:.3f}", (20, 160),
                        font, 0.5, (255, 255, 255), 1)

        # å¸§æ•°è®¡æ•°
        cv2.putText(frame, f"Frame: {self.frame_count}", (300, height - 20),
                    font, 0.4, (255, 255, 0), 1)

        return frame

    # =================== ä¸»è¦è¿è¡Œæ¥å£ ===================

    def start_camera_recognition(self, camera_index: int = 0):
        """å¼€å§‹æ‘„åƒå¤´è¯†åˆ« - å…¼å®¹main.pyçš„æ¥å£"""
        if self.is_running:
            print("âš ï¸ è§†è§‰è¯†åˆ«å·²åœ¨è¿è¡Œä¸­")
            return

        print(f"ğŸš€ å¯åŠ¨è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿï¼ˆæ‘„åƒå¤´ {camera_index}ï¼‰")

        self.should_stop = False
        self.is_running = True

        def recognition_worker():
            try:
                # åˆå§‹åŒ–æ‘„åƒå¤´
                self.camera_cap = cv2.VideoCapture(camera_index)
                if not self.camera_cap.isOpened():
                    print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
                    self.is_running = False
                    return

                # æ‘„åƒå¤´é…ç½®
                self.camera_cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                self.camera_cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                self.camera_cap.set(cv2.CAP_PROP_FPS, 30)

                print("âœ… è§†è§‰è¯†åˆ«å¯åŠ¨æˆåŠŸï¼Œå¼€å§‹å¤„ç†è§†é¢‘æµ...")

                while self.is_running and not self.should_stop:
                    ret, frame = self.camera_cap.read()
                    if not ret:
                        print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                        break

                    # å¤„ç†å¸§
                    processed_frame = self.process_frame(frame)

                    # åœ¨é›†æˆæ¨¡å¼ä¸‹ï¼Œä¸æ˜¾ç¤ºçª—å£ï¼Œåªå¤„ç†æ•°æ®
                    # å¦‚æœéœ€è¦è°ƒè¯•ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç 
                    # if processed_frame is not None:
                    #     cv2.imshow("è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«", processed_frame)
                    #     if cv2.waitKey(1) & 0xFF == 27:  # ESCé€€å‡º
                    #         break

                    time.sleep(0.033)  # çº¦30fps

            except Exception as e:
                print(f"âŒ è§†è§‰è¯†åˆ«è¿è¡Œé”™è¯¯: {e}")
            finally:
                self.cleanup()

        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œè¯†åˆ«
        self.vision_thread = threading.Thread(target=recognition_worker, daemon=True)
        self.vision_thread.start()

    def stop(self):
        """åœæ­¢è¯†åˆ«ç³»ç»Ÿ - å…¼å®¹main.pyçš„æ¥å£"""
        print("ğŸ›‘ åœæ­¢è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿ")
        self.should_stop = True
        self.is_running = False

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.vision_thread and self.vision_thread.is_alive():
            self.vision_thread.join(timeout=2)

        self.cleanup()

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.camera_cap:
                self.camera_cap.release()
                self.camera_cap = None

            cv2.destroyAllWindows()

            # é‡ç½®çŠ¶æ€
            self.current_frame = None
            self.is_running = False

        except Exception as e:
            print(f"æ¸…ç†è§†è§‰è¯†åˆ«èµ„æºæ—¶å‡ºé”™: {e}")

    # =================== è°ƒè¯•å’Œæµ‹è¯•æ¥å£ ===================

    def start_recognition_with_display(self, camera_index: int = 0):
        """å¯åŠ¨å¸¦æ˜¾ç¤ºç•Œé¢çš„è¯†åˆ«ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        print(f"\nğŸš€ å¯åŠ¨è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ç³»ç»Ÿï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰")

        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
            return

        # æ‘„åƒå¤´é…ç½®
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)

        cv2.namedWindow("è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰", cv2.WINDOW_NORMAL)
        self.is_running = True

        try:
            while self.is_running:
                ret, frame = cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    break

                # å¤„ç†å¸§
                processed_frame = self.process_frame(frame)
                if processed_frame is not None:
                    cv2.imshow("è½¦è½½æ™ºèƒ½è§†è§‰è¯†åˆ«ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰", processed_frame)

                # æŒ‰é”®æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC
                    break
                elif key == ord('q'):
                    break

        except KeyboardInterrupt:
            print("ç”¨æˆ·ä¸­æ–­")
        finally:
            self.stop()
            cap.release()


# =================== æµ‹è¯•å’Œæ¼”ç¤º ===================

def test_vision_system():
    """æµ‹è¯•è½¦è½½è§†è§‰è¯†åˆ«ç³»ç»Ÿ"""

    def command_callback(cmd_type, cmd_text):
        print(f"ğŸ¯ ç³»ç»Ÿæ¥æ”¶æŒ‡ä»¤: [{cmd_type}] {cmd_text}")

    vision = VisionRecognition(command_callback)

    try:
        # ä½¿ç”¨è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼ˆå¸¦æ˜¾ç¤ºç•Œé¢ï¼‰
        vision.start_recognition_with_display()
    except KeyboardInterrupt:
        print("æµ‹è¯•ç»“æŸ")
    finally:
        vision.stop()


if __name__ == "__main__":
    test_vision_system()